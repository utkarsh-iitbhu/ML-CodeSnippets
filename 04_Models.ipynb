{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Model Selection and Training**\n",
    "   - 1.1 Classification Models\n",
    "      - Logistic Regression\n",
    "      - Decision Trees\n",
    "      - Random Forest\n",
    "      - Support Vector Machines (SVM)\n",
    "      - Gradient Boosting (e.g., XGBoost, LightGBM)\n",
    "      - K-Nearest Neighbors (KNN)\n",
    "      - Naive Bayes\n",
    "      - Neural Networks\n",
    "\n",
    "   - 1.2 Regression Models\n",
    "      - Linear Regression\n",
    "      - Polynomial Regression\n",
    "      - Ridge Regression\n",
    "      - Lasso Regression\n",
    "      - Elastic Net\n",
    "      - Decision Tree Regressor\n",
    "      - Random Forest Regressor\n",
    "      - Gradient Boosting Regressor\n",
    "      - Support Vector Regression (SVR)\n",
    "\n",
    "   - 1.3 Clustering Models\n",
    "      - K-Means\n",
    "      - Hierarchical Clustering\n",
    "      - DBSCAN\n",
    "      - Gaussian Mixture Models\n",
    "      - Mean Shift\n",
    "      - Spectral Clustering\n",
    "\n",
    "   - 1.4 Cross-Validation Techniques\n",
    "      - K-Fold Cross-Validation\n",
    "      - Stratified K-Fold Cross-Validation\n",
    "      - Leave-One-Out Cross-Validation\n",
    "      - Time Series Cross-Validation\n",
    "\n",
    "   - 1.5 Handling Class Imbalance\n",
    "      - Oversampling (e.g., SMOTE)\n",
    "      - Undersampling\n",
    "      - Combination (SMOTEENN, SMOTETomek)\n",
    "      - Class Weights\n",
    "      - Ensemble Methods (e.g., BalancedRandomForestClassifier)\n",
    "\n",
    "**2. Hyperparameter Tuning:**\n",
    "   - 2.1 Grid Search\n",
    "   - 2.2 Random Search\n",
    "   - 2.3 Bayesian Optimization\n",
    "   - 2.4 Genetic Algorithms\n",
    "   - 2.5 Hyperband\n",
    "   - 2.6 Optuna\n",
    "\n",
    "**3. Model Evaluation:**\n",
    "   - 3.1 Classification Metrics\n",
    "      - Accuracy\n",
    "      - Precision\n",
    "      - Recall\n",
    "      - F1-Score\n",
    "      - ROC-AUC\n",
    "      - PR-AUC\n",
    "\n",
    "   - 3.2 Regression Metrics\n",
    "      - Mean Squared Error (MSE)\n",
    "      - Root Mean Squared Error (RMSE)\n",
    "      - Mean Absolute Error (MAE)\n",
    "      - R-squared (R2)\n",
    "      - Adjusted R-squared\n",
    "\n",
    "   - 3.3 Clustering Metrics\n",
    "      - Silhouette Score\n",
    "      - Calinski-Harabasz Index\n",
    "      - Davies-Bouldin Index\n",
    "\n",
    "   - 3.4 Reports and Visualizations\n",
    "      - Confusion Matrix\n",
    "      - Classification Report\n",
    "      - ROC Curve\n",
    "      - Precision-Recall Curve\n",
    "      - Learning Curves\n",
    "      - Validation Curves\n",
    "\n",
    "**4. Model Interpretation:**\n",
    "   - 4.1 Feature Importance Analysis\n",
    "      - Random Forest Feature Importance\n",
    "      - Permutation Importance\n",
    "      - Recursive Feature Elimination (RFE)\n",
    "\n",
    "   - 4.2 SHAP (SHapley Additive exPlanations)\n",
    "      - SHAP Summary Plot\n",
    "      - SHAP Dependence Plot\n",
    "      - SHAP Force Plot\n",
    "      - SHAP Interaction Values\n",
    "\n",
    "   - 4.3 Partial Dependence Plots (PDP)\n",
    "   - 4.4 Individual Conditional Expectation (ICE) Plots\n",
    "   - 4.5 Global Surrogate Models\n",
    "   - 4.6 Local Interpretable Model-agnostic Explanations (LIME)\n",
    "\n",
    "**5. Ensemble Methods:**\n",
    "   - 5.1 Bagging\n",
    "      - Random Forest\n",
    "      - Bagging Classifier/Regressor\n",
    "\n",
    "   - 5.2 Boosting\n",
    "      - AdaBoost\n",
    "      - Gradient Boosting\n",
    "      - XGBoost\n",
    "      - LightGBM\n",
    "      - CatBoost\n",
    "\n",
    "   - 5.3 Stacking\n",
    "      - StackingClassifier\n",
    "      - StackingRegressor\n",
    "\n",
    "   - 5.4 Voting\n",
    "      - VotingClassifier\n",
    "      - VotingRegressor\n",
    "\n",
    "_____________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classification_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Dictionary of classifiers\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(),  # Linear model for binary or multiclass classification based on the log-odds.\n",
    "    'Decision Tree': DecisionTreeClassifier(),  # Non-linear model that splits data into branches to fit a piecewise constant function.\n",
    "    'Random Forest': RandomForestClassifier(),  # Ensemble of decision trees that reduces overfitting by averaging multiple trees.\n",
    "    'SVM': SVC(),  # Support Vector Classification that finds the optimal hyperplane separating different classes.\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),  # Ensemble technique that builds trees sequentially to correct errors of previous trees.\n",
    "    'KNN': KNeighborsClassifier(),  # Instance-based learning method that classifies samples based on the majority class of their nearest neighbors.\n",
    "    'Naive Bayes': GaussianNB(),  # Probabilistic classifier based on Bayes' theorem with an assumption of feature independence.\n",
    "    'Neural Network': MLPClassifier()  # Multi-layer Perceptron that uses multiple layers of neurons to model complex patterns in data.\n",
    "}\n",
    "\n",
    "# You can easily switch classifiers like this:\n",
    "# classification_pipeline.set_params(classifier=classifiers['Random Forest'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "regression_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Dictionary of regressors\n",
    "regressors = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Polynomial Regression': Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=2)),  # Transforms features into polynomial features of a specified degree.\n",
    "        ('linear', LinearRegression())  # Fits a linear model to the polynomial-transformed features.\n",
    "    ]),\n",
    "    'Ridge Regression': Ridge(),  # Linear model with L2 regularization to prevent overfitting by penalizing large coefficients.\n",
    "    'Lasso Regression': Lasso(),  # Linear model with L1 regularization to enforce sparsity by penalizing the absolute values of coefficients.\n",
    "    'Elastic Net': ElasticNet(),  # Combines L1 and L2 regularization to enforce sparsity and prevent overfitting.\n",
    "    'Decision Tree Regressor': DecisionTreeRegressor(),  # Non-linear model that splits data into branches to fit a piecewise constant function.\n",
    "    'Random Forest Regressor': RandomForestRegressor(),  # Ensemble of decision trees that reduces overfitting by averaging multiple trees.\n",
    "    'Gradient Boosting Regressor': GradientBoostingRegressor(),  # Ensemble technique that builds trees sequentially to correct errors of previous trees.\n",
    "    'SVR': SVR()  # Support Vector Regression fits the data within a margin while minimizing model complexity.\n",
    "}\n",
    "\n",
    "# You can easily switch regressors like this:\n",
    "# regression_pipeline.set_params(regressor=regressors['Random Forest Regressor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, MeanShift, SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "clustering_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clusterer', KMeans())  # Default clustering algorithm (K-Means).\n",
    "])\n",
    "\n",
    "# Dictionary of clustering algorithms\n",
    "clusterers = {\n",
    "    'K-Means': KMeans(),  # Partitions data into k clusters by minimizing the variance within each cluster.\n",
    "    'Hierarchical Clustering': AgglomerativeClustering(),  # Builds a hierarchy of clusters by merging or splitting them successively.\n",
    "    'DBSCAN': DBSCAN(),  # Density-Based Spatial Clustering of Applications with Noise; finds core samples of high density and expands clusters from them.\n",
    "    'Gaussian Mixture Models': GaussianMixture(),  # Probabilistic model that assumes data is generated from a mixture of several Gaussian distributions.\n",
    "    'Mean Shift': MeanShift(),  # Clustering algorithm that assigns data points to the nearest cluster center with the highest density.\n",
    "    'Spectral Clustering': SpectralClustering()  # Uses the eigenvalues of a similarity matrix to perform dimensionality reduction before clustering.\n",
    "}\n",
    "\n",
    "# You can easily switch clusterers like this:\n",
    "# clustering_pipeline.set_params(clusterer=clusterers['DBSCAN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4 Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Use it to get an estimate of the model's accuracy and its variance.\n",
    "def perform_cross_validation(pipeline, X, y, cv=5, scoring='accuracy'):\n",
    "    scores = cross_val_score(pipeline, X, y, cv=cv, scoring=scoring)\n",
    "    return scores.mean(), scores.std()\n",
    "\n",
    "# Use it to optimize model parameters for better performance.\n",
    "def perform_grid_search(pipeline, param_grid, X, y, cv=5, scoring='accuracy'):\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring=scoring)\n",
    "    grid_search.fit(X, y)\n",
    "    return grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "# Use it as a more efficient alternative to grid search when the parameter space is large.\n",
    "def perform_randomized_search(pipeline, param_distributions, X, y, n_iter=10, cv=5, scoring='accuracy'):\n",
    "    random_search = RandomizedSearchCV(pipeline, param_distributions, n_iter=n_iter, cv=cv, scoring=scoring)\n",
    "    random_search.fit(X, y)\n",
    "    return random_search.best_params_, random_search.best_score_\n",
    "\n",
    "# Use it to ensure each fold has the same proportion of class labels, which is especially useful for imbalanced datasets.\n",
    "def perform_stratified_k_fold_cross_validation(pipeline, X, y, n_splits=5, scoring='accuracy'):\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    scores = cross_val_score(pipeline, X, y, cv=skf, scoring=scoring)\n",
    "    return scores.mean(), scores.std()\n",
    "\n",
    "# Use it to get detailed performance metrics such as precision, recall, and F1-score.\n",
    "def evaluate_classifier(pipeline, X_test, y_test):\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Use it to get regression performance metrics such as Mean Squared Error (MSE) and R-squared.\n",
    "def evaluate_regressor(pipeline, X_test, y_test):\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"R-squared Score: {r2}\")\n",
    "\n",
    "# Example usage:\n",
    "# cv_score, cv_std = perform_cross_validation(classification_pipeline, X, y)\n",
    "# best_params, best_score = perform_grid_search(classification_pipeline, param_grid, X, y)\n",
    "# best_params, best_score = perform_randomized_search(classification_pipeline, param_distributions, X, y)\n",
    "# cv_score, cv_std = perform_stratified_k_fold_cross_validation(classification_pipeline, X, y)\n",
    "# evaluate_classifier(classification_pipeline, X_test, y_test)\n",
    "# evaluate_regressor(regression_pipeline, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5 Class Imbalance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# You can add these to your pipeline like this:\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "imbalanced_pipeline = ImbPipeline([\n",
    "    ('sampler', SMOTE()),  # Synthetic Minority Over-sampling Technique: Generates synthetic samples for the minority class.\n",
    "    ('scaler', StandardScaler()),  # Standardizes features by removing the mean and scaling to unit variance.\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Other sampling techniques\n",
    "samplers = {\n",
    "    'SMOTE': SMOTE(),  # Generates synthetic samples for the minority class.\n",
    "    'Random Undersampling': RandomUnderSampler(),  # Randomly removes samples from the majority class.\n",
    "    'SMOTEENN': SMOTEENN(),  # Combines SMOTE and Edited Nearest Neighbors for resampling.\n",
    "    'SMOTETomek': SMOTETomek()  # Combines SMOTE and Tomek links for resampling.\n",
    "}\n",
    "\n",
    "# Balanced Random Forest\n",
    "balanced_rf = BalancedRandomForestClassifier()  # Random Forest with balanced class sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Hyperparametertuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "u:\\ML-Snippets\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\utkar\\AppData\\Local\\Temp\\ipykernel_2544\\158980399.py:7: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner import Hyperband, HyperParameters\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np \n",
    "from skopt import BayesSearchCV # type: ignore\n",
    "from deap import base, creator, tools, algorithms\n",
    "import optuna # type: ignore\n",
    "from kerastuner import Hyperband, HyperParameters # type: ignore\n",
    "\n",
    "# 2.1 Grid Search\n",
    "# param_grid: Dictionary with parameters names as keys and lists of parameter settings to try.\n",
    "def grid_search(estimator, param_grid, X, y, cv=5):\n",
    "    grid_search = GridSearchCV(estimator, param_grid, cv=cv, n_jobs=-1)\n",
    "    grid_search.fit(X, y)\n",
    "    return grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "# 2.2 Random Search\n",
    "# Randomly samples from a range of hyperparameter values to find the best combination, often faster than grid search.\n",
    "def random_search(estimator, param_distributions, X, y, cv=5, n_iter=10):\n",
    "    random_search = RandomizedSearchCV(estimator, param_distributions, n_iter=n_iter, cv=cv, n_jobs=-1)\n",
    "    random_search.fit(X, y)\n",
    "    return random_search.best_params_, random_search.best_score_\n",
    "\n",
    "# 2.3 Bayesian Optimization\n",
    "# Probabilistic models to find the best hyperparameters by iteratively exploring promising regions of the hyperparameter space.\n",
    "def bayesian_optimization(estimator, search_spaces, X, y, cv=5, n_iter=50):\n",
    "    bayes_search = BayesSearchCV(estimator, search_spaces, n_iter=n_iter, cv=cv, n_jobs=-1)\n",
    "    bayes_search.fit(X, y)\n",
    "    return bayes_search.best_params_, bayes_search.best_score_\n",
    "\n",
    "# 2.4 Genetic Algorithm\n",
    "# Optimizes hyperparameters using techniques as mutation and crossover to evolve better parameter sets over generations.\n",
    "def genetic_algorithm(estimator, param_grid, X, y, cv=5, population_size=50, generations=10):\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "    toolbox = base.Toolbox()\n",
    "    for key, values in param_grid.items():\n",
    "        toolbox.register(f\"attr_{key}\", np.random.choice, values)\n",
    "    \n",
    "    toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                     [getattr(toolbox, f\"attr_{key}\") for key in param_grid.keys()], n=1)\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "    def evaluate(individual):\n",
    "        params = dict(zip(param_grid.keys(), individual))\n",
    "        estimator.set_params(**params)\n",
    "        return np.mean(cross_val_score(estimator, X, y, cv=cv)),\n",
    "\n",
    "    toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "    toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "    toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "    population = toolbox.population(n=population_size)\n",
    "    result, _ = algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=generations, verbose=False)\n",
    "    \n",
    "    best_individual = tools.selBest(result, k=1)[0]\n",
    "    best_params = dict(zip(param_grid.keys(), best_individual))\n",
    "    best_score = evaluate(best_individual)[0]\n",
    "    \n",
    "    return best_params, best_score\n",
    "\n",
    "# 2.5 Hyperband\n",
    "# Adaptive resource allocation to quickly find good hyperparameters by evaluating many configurations with limited resources.\n",
    "def hyperband_tuning(build_model, hp, X, y, max_epochs=50, factor=3, hyperband_iterations=1):\n",
    "    tuner = Hyperband(\n",
    "        build_model,\n",
    "        objective='val_accuracy',\n",
    "        max_epochs=max_epochs,\n",
    "        factor=factor,\n",
    "        hyperparameters=hp,\n",
    "        directory='hyperband_dir',\n",
    "        project_name='hyperband_tuning'\n",
    "    )\n",
    "    tuner.search(X, y, epochs=max_epochs, validation_split=0.2)\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    return best_hps.values, tuner.get_best_models()[0]\n",
    "\n",
    "# 2.6 Optuna\n",
    "# Employs tree-based algorithms to efficiently explore hyperparameter spaces and find the optimal configuration through a series of trials.\n",
    "def optuna_tuning(objective, n_trials=100):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study.best_params, study.best_value\n",
    "\n",
    "# Example usage:\n",
    "# best_params, best_score = grid_search(estimator, param_grid, X, y)\n",
    "# best_params, best_score = random_search(estimator, param_distributions, X, y)\n",
    "# best_params, best_score = bayesian_optimization(estimator, search_spaces, X, y)\n",
    "# best_params, best_score = genetic_algorithm(estimator, param_grid, X, y)\n",
    "\n",
    "# For Hyperband (requires TensorFlow and Keras):\n",
    "# def build_model(hp):\n",
    "#     model = keras.Sequential()\n",
    "#     model.add(keras.layers.Dense(units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "#                                  activation='relu'))\n",
    "#     model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "# hp = HyperParameters()\n",
    "# best_hps, best_model = hyperband_tuning(build_model, hp, X, y)\n",
    "\n",
    "# For Optuna:\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "#         'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "#         'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "#     }\n",
    "#     model = RandomForestClassifier(**params)\n",
    "#     return np.mean(cross_val_score(model, X, y, cv=5))\n",
    "# best_params, best_score = optuna_tuning(objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, average_precision_score,\n",
    "                             mean_squared_error, mean_absolute_error, r2_score,\n",
    "                             silhouette_score, calinski_harabasz_score, davies_bouldin_score,\n",
    "                             confusion_matrix, classification_report, roc_curve, precision_recall_curve)\n",
    "from sklearn.model_selection import learning_curve, validation_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 3.1 Classification Metrics\n",
    "def classification_metrics(y_true, y_pred, y_prob=None):\n",
    "    # Computes classification metrics: Accuracy, Precision, Recall, and F1-Score\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),  # Ratio of correctly predicted instances\n",
    "        'Precision': precision_score(y_true, y_pred, average='weighted'),  # Ratio of true positives to total predicted positives\n",
    "        'Recall': recall_score(y_true, y_pred, average='weighted'),  # Ratio of true positives to total actual positives\n",
    "        'F1-Score': f1_score(y_true, y_pred, average='weighted')  # Harmonic mean of Precision and Recall\n",
    "    }\n",
    "    \n",
    "    if y_prob is not None:\n",
    "        metrics['ROC-AUC'] = roc_auc_score(y_true, y_prob, average='weighted', multi_class='ovr')  # Area under the ROC curve\n",
    "        metrics['PR-AUC'] = average_precision_score(y_true, y_prob, average='weighted')  # Area under the Precision-Recall curve\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# 3.2 Regression Metrics\n",
    "def regression_metrics(y_true, y_pred):\n",
    "    # Computes regression metrics: MSE, RMSE, MAE, and R-squared\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    return {\n",
    "        'MSE': mse,  # Mean Squared Error\n",
    "        'RMSE': np.sqrt(mse),  # Root Mean Squared Error\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),  # Mean Absolute Error\n",
    "        'R-squared': r2_score(y_true, y_pred),  # Proportion of variance explained by the model\n",
    "        'Adjusted R-squared': 1 - (1-r2_score(y_true, y_pred))*(len(y_true)-1)/(len(y_true)-len(y_pred.shape)-1)  # Adjusted for number of predictors\n",
    "    }\n",
    "\n",
    "# 3.3 Clustering Metrics\n",
    "def clustering_metrics(X, labels):\n",
    "    # Evaluates clustering results: Silhouette Score, Calinski-Harabasz Index, Davies-Bouldin Index\n",
    "    return {\n",
    "        'Silhouette Score': silhouette_score(X, labels),  # Measures how similar an instance is to its own cluster vs. other clusters\n",
    "        'Calinski-Harabasz Index': calinski_harabasz_score(X, labels),  # Ratio of the sum of between-cluster dispersion to within-cluster dispersion\n",
    "        'Davies-Bouldin Index': davies_bouldin_score(X, labels)  # Average similarity ratio of each cluster with its most similar cluster\n",
    "    }\n",
    "\n",
    "# 3.4 Reports and Visualizations\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    # Plots a confusion matrix to visualize classification performance\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_classification_report(y_true, y_pred):\n",
    "    # Generates and visualizes a classification report as a heatmap\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.heatmap(df_report.iloc[:-1, :-1], annot=True, cmap=\"Blues\", fmt='.2f')\n",
    "    plt.title('Classification Report')\n",
    "    plt.ylabel('Classes')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(y_true, y_prob):\n",
    "    # Plots the ROC curve for binary classification\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.show()\n",
    "\n",
    "def plot_pr_curve(y_true, y_prob):\n",
    "    # Plots the Precision-Recall curve\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.show()\n",
    "\n",
    "def plot_learning_curve(estimator, X, y, cv=5):\n",
    "    # Displays the learning curve showing training and cross-validation scores\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, np.mean(train_scores, axis=1), label='Training score')\n",
    "    plt.plot(train_sizes, np.mean(test_scores, axis=1), label='Cross-validation score')\n",
    "    plt.xlabel('Training examples')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_validation_curve(estimator, X, y, param_name, param_range, cv=5):\n",
    "    # Plots the validation curve to visualize the effect of different parameter values\n",
    "    train_scores, test_scores = validation_curve(\n",
    "        estimator, X, y, param_name=param_name, param_range=param_range, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(param_range, np.mean(train_scores, axis=1), label=\"Training score\")\n",
    "    plt.plot(param_range, np.mean(test_scores, axis=1), label=\"Cross-validation score\")\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Validation Curve')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# class_metrics = classification_metrics(y_true, y_pred, y_prob)\n",
    "# reg_metrics = regression_metrics(y_true, y_pred)\n",
    "# clust_metrics = clustering_metrics(X, labels)\n",
    "# plot_confusion_matrix(y_true, y_pred)\n",
    "# plot_classification_report(y_true, y_pred)\n",
    "# plot_roc_curve(y_true, y_prob)\n",
    "# plot_pr_curve(y_true, y_prob)\n",
    "# plot_learning_curve(estimator, X, y)\n",
    "# plot_validation_curve(estimator, X, y, 'max_depth', range(1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Model Interpretation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "u:\\ML-Snippets\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay \n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import shap\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Feature Importance\n",
    "def get_feature_importance(pipeline, X, y):\n",
    "    # Extracts feature importances or coefficients from the model, or uses permutation importance\n",
    "    model = pipeline.named_steps['classifier'] if 'classifier' in pipeline.named_steps else pipeline.named_steps['regressor']\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        return model.feature_importances_  # Feature importances for tree-based models\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        return model.coef_  # Coefficients for linear models\n",
    "    else:\n",
    "        return permutation_importance(pipeline, X, y).importances_mean  # Permutation importance for other models\n",
    "\n",
    "# Recursive Feature Elimination (RFE)\n",
    "def perform_rfe(pipeline, X, y, n_features_to_select=5):\n",
    "    # Selects features by recursively removing the least important ones\n",
    "    model = pipeline.named_steps['classifier'] if 'classifier' in pipeline.named_steps else pipeline.named_steps['regressor']\n",
    "    rfe = RFE(estimator=model, n_features_to_select=n_features_to_select)\n",
    "    rfe.fit(X, y)\n",
    "    return rfe.support_, rfe.ranking_  # Support (selected features) and ranking of features\n",
    "\n",
    "# SHAP Values\n",
    "def plot_shap_values(pipeline, X):\n",
    "    # Plots SHAP values for model interpretability\n",
    "    explainer = shap.Explainer(pipeline.named_steps['classifier'], X) if 'classifier' in pipeline.named_steps else shap.Explainer(pipeline.named_steps['regressor'], X)\n",
    "    shap_values = explainer(X)\n",
    "    shap.summary_plot(shap_values, X)  # Summary plot of SHAP values\n",
    "    shap.dependence_plot(0, shap_values, X)  # Dependence plot for the first feature\n",
    "    shap.force_plot(explainer.expected_value, shap_values[0], X.iloc[0])  # Force plot for the first instance\n",
    "    shap_interaction_values = explainer.shap_interaction_values(X)\n",
    "    shap.summary_plot(shap_interaction_values, X)  # Summary plot of SHAP interaction values\n",
    "\n",
    "# Partial Dependence Plots (PDP)\n",
    "def plot_pdp(pipeline, X, features):\n",
    "    # Plots Partial Dependence Plots to show effect of features on predictions\n",
    "    PartialDependenceDisplay(pipeline, X, features)\n",
    "    plt.show()\n",
    "\n",
    "# Individual Conditional Expectation (ICE) Plots\n",
    "def plot_ice(pipeline, X, feature):\n",
    "    # Plots Individual Conditional Expectation (ICE) plots for a specific feature\n",
    "    ice_plot = PartialDependenceDisplay(pipeline, X, [feature], kind='both')\n",
    "    plt.show()\n",
    "\n",
    "# Global Surrogate Models\n",
    "def fit_global_surrogate(pipeline, X, y):\n",
    "    # Fits a global surrogate model (Decision Tree) to approximate the predictions of the pipeline\n",
    "    model = pipeline.named_steps['classifier'] if 'classifier' in pipeline.named_steps else pipeline.named_steps['regressor']\n",
    "    surrogate = DecisionTreeClassifier(max_depth=3) if 'classifier' in pipeline.named_steps else DecisionTreeRegressor(max_depth=3)\n",
    "    surrogate.fit(X, model.predict(X))\n",
    "    return surrogate\n",
    "\n",
    "# LIME\n",
    "def explain_with_lime(pipeline, X, y, idx=0):\n",
    "    # Uses LIME to explain a single instance by approximating the model with an interpretable model\n",
    "    explainer = LimeTabularExplainer(X.values, feature_names=X.columns, class_names=['class_0', 'class_1'], mode='classification') if 'classifier' in pipeline.named_steps else LimeTabularExplainer(X.values, feature_names=X.columns, mode='regression')\n",
    "    exp = explainer.explain_instance(X.iloc[idx], pipeline.predict, num_features=5)\n",
    "    exp.show_in_notebook()\n",
    "\n",
    "# Example usage:\n",
    "# importances = get_feature_importance(classification_pipeline, X_train_class, y_train_class)\n",
    "# print(\"Feature Importances:\", importances)\n",
    "\n",
    "# rfe_support, rfe_ranking = perform_rfe(classification_pipeline, X_train_class, y_train_class)\n",
    "# print(\"RFE Support:\", rfe_support)\n",
    "# print(\"RFE Ranking:\", rfe_ranking)\n",
    "\n",
    "# plot_shap_values(classification_pipeline, X_train_class)\n",
    "\n",
    "# plot_pdp(classification_pipeline, X_train_class, [0, 1])  # Example for features at index 0 and 1\n",
    "# plot_ice(classification_pipeline, X_train_class, 0)  # Example for feature at index 0\n",
    "\n",
    "# surrogate_model = fit_global_surrogate(classification_pipeline, X_train_class, y_train_class)\n",
    "# print(\"Global Surrogate Model:\", surrogate_model)\n",
    "\n",
    "# explain_with_lime(classification_pipeline, X_train_class, y_train_class, idx=0)  # Example for instance at index 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Ensemble Methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (BaggingClassifier, BaggingRegressor, \n",
    "                              AdaBoostClassifier, AdaBoostRegressor, \n",
    "                              StackingClassifier, StackingRegressor, \n",
    "                              VotingClassifier, VotingRegressor)\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.ensemble import (RandomForestClassifier, RandomForestRegressor, \n",
    "                              GradientBoostingClassifier, GradientBoostingRegressor)\n",
    "\n",
    "# Bagging: \n",
    "bagging_classifier = BaggingClassifier(estimator=DecisionTreeClassifier()) # Combines multiple decision trees to improve classification performance by averaging their predictions.\n",
    "bagging_regressor = BaggingRegressor(estimator=DecisionTreeRegressor()) # Combines multiple decision trees to improve regression performance by averaging their predictions.\n",
    "\n",
    "# Boosting: \n",
    "adaboost_classifier = AdaBoostClassifier() # Boosts weak classifiers (like decision trees) by focusing on the mistakes of previous models.\n",
    "adaboost_regressor = AdaBoostRegressor() # Boosts weak regressors to improve predictions by focusing on errors of previous models.\n",
    "\n",
    "xgboost_classifier = XGBClassifier() # Implements gradient boosting with a more efficient and scalable approach, often providing high performance.\n",
    "xgboost_regressor = XGBRegressor() # Implements gradient boosting for regression with high efficiency and performance.\n",
    "\n",
    "lightgbm_classifier = LGBMClassifier() # Uses gradient boosting with a focus on speed and performance, especially for large datasets.\n",
    "lightgbm_regressor = LGBMRegressor() # Uses gradient boosting for regression, optimized for efficiency and scalability.\n",
    "\n",
    "\n",
    "# Stacking: \n",
    "stacking_classifier = StackingClassifier( # Combines predictions from multiple classifiers (RF, SVM, GB) using a logistic regression model as the final predictor.\n",
    "    estimators=[('rf', RandomForestClassifier()),\n",
    "                ('svm', SVC()),\n",
    "                ('gb', GradientBoostingClassifier())],\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "\n",
    "stacking_regressor = StackingRegressor( # Combines predictions from multiple regressors (RF, SVR, GB) using linear regression as the final predictor.\n",
    "    estimators=[('rf', RandomForestRegressor()),\n",
    "                ('svr', SVR()),\n",
    "                ('gb', GradientBoostingRegressor())],\n",
    "    final_estimator=LinearRegression()\n",
    ")\n",
    "\n",
    "# Voting\n",
    "voting_classifier = VotingClassifier( # Combines multiple classifiers (LR, RF, SVM) by taking a majority vote to make the final prediction.\n",
    "    estimators=[('lr', LogisticRegression()),\n",
    "                ('rf', RandomForestClassifier()),\n",
    "                ('svm', SVC())],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "voting_regressor = VotingRegressor( # Combines multiple regressors (LR, RF, SVR) by averaging their predictions to make the final prediction.\n",
    "    estimators=[('lr', LinearRegression()),\n",
    "                ('rf', RandomForestRegressor()),\n",
    "                ('svr', SVR())]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, mean_squared_error\n",
    "\n",
    "# Load datasets\n",
    "classification_data = load_iris()\n",
    "regression_data = fetch_california_housing()\n",
    "X_class = classification_data.data\n",
    "y_class = classification_data.target\n",
    "X_reg = regression_data.data\n",
    "y_reg = regression_data.target\n",
    "\n",
    "# Split datasets\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X_class, y_class, test_size=0.2, random_state=0)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=0)\n",
    "\n",
    "# Define pipelines for classification\n",
    "classification_pipeline = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer([\n",
    "        ('scaler', StandardScaler(), [0, 1, 2, 3])  # Assuming all features are numeric\n",
    "    ])),\n",
    "    ('classifier', LogisticRegression())  # Default model\n",
    "])\n",
    "\n",
    "# Define pipelines for regression\n",
    "regression_pipeline = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer([\n",
    "        ('scaler', StandardScaler(), [0, 1, 2, 3])  # Assuming all features are numeric\n",
    "    ])),\n",
    "    ('regressor', LinearRegression())  # Default model\n",
    "])\n",
    "\n",
    "# Combine all models into a single pipeline with a model selector\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "class ModelSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return self.model.score(X, y)\n",
    "\n",
    "# Define a function to create a model pipeline\n",
    "def create_model_pipeline(model, model_type):\n",
    "    return Pipeline([\n",
    "        ('preprocessor', ColumnTransformer([\n",
    "            ('scaler', StandardScaler(), list(range(X_train_class.shape[1])))\n",
    "        ])),\n",
    "        (model_type, ModelSelector(model))\n",
    "    ])\n",
    "\n",
    "# Create pipelines for all models\n",
    "model_pipelines = {\n",
    "    'Logistic Regression': create_model_pipeline(LogisticRegression(), 'classifier'),\n",
    "    'Random Forest Classifier': create_model_pipeline(RandomForestClassifier(), 'classifier'),\n",
    "    'Support Vector Machines': create_model_pipeline(SVC(), 'classifier'),\n",
    "    'Gradient Boosting Classifier': create_model_pipeline(GradientBoostingClassifier(), 'classifier'),\n",
    "    'K-Nearest Neighbors': create_model_pipeline(KNeighborsClassifier(), 'classifier'),\n",
    "    'Naive Bayes': create_model_pipeline(GaussianNB(), 'classifier'),\n",
    "    'Linear Regression': create_model_pipeline(LinearRegression(), 'regressor'),\n",
    "    'Ridge Regression': create_model_pipeline(Ridge(), 'regressor'),\n",
    "    'Lasso Regression': create_model_pipeline(Lasso(), 'regressor'),\n",
    "    'Elastic Net': create_model_pipeline(ElasticNet(), 'regressor'),\n",
    "    'Decision Tree Regressor': create_model_pipeline(DecisionTreeRegressor(), 'regressor'),\n",
    "    'Random Forest Regressor': create_model_pipeline(RandomForestRegressor(), 'regressor'),\n",
    "    'Gradient Boosting Regressor': create_model_pipeline(GradientBoostingRegressor(), 'regressor'),\n",
    "    'Support Vector Regression': create_model_pipeline(SVR(), 'regressor')\n",
    "}\n",
    "\n",
    "# Train and evaluate classification models\n",
    "for name, pipeline in model_pipelines.items():\n",
    "    if 'classifier' in name:\n",
    "        pipeline.fit(X_train_class, y_train_class)\n",
    "        preds = pipeline.predict(X_test_class)\n",
    "        print(f\"{name} Classification Report:\")\n",
    "        print(classification_report(y_test_class, preds))\n",
    "\n",
    "# Train and evaluate regression models\n",
    "for name, pipeline in model_pipelines.items():\n",
    "    if 'regressor' in name:\n",
    "        pipeline.fit(X_train_reg, y_train_reg)\n",
    "        preds = pipeline.predict(X_test_reg)\n",
    "        print(f\"{name} Mean Squared Error:\")\n",
    "        print(mean_squared_error(y_test_reg, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
